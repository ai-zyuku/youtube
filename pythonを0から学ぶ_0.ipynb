{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【内容】\n",
    "\n",
    "・データ読み込み\n",
    "\n",
    "・目的変数と説明変数のテーブルデータに変換\n",
    "\n",
    "・初期ハイパーパラメータ初期設定\n",
    "\n",
    "・グリッドリサーチでハイパーパラメータ設定\n",
    "\n",
    "・アンサンブル学習(https://potesara-tips.com/ensemble-voting/#toc10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer#サンプルデータセットのライブラリ\n",
    "import pandas as pd#テーブルデータや時系列データを操作するためのデータ構造と演算のライブラリ\n",
    "\n",
    "data = load_breast_cancer()#データ読み込み\n",
    "data#dataを表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目的変数と説明変数のテーブルデータに変換\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = pd.DataFrame(data=data.data,columns=data.feature_names)#説明変数のテーブルデータに変換。データ:data,カラム:columns\n",
    "train_x.head()#train_xのデータフレームの最初の5行表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = pd.DataFrame(data=data.target,columns=['class'])#目的変数のテーブルデータに変換。データ:data,カラム:columns\n",
    "train_y.head()#train_yのデータフレームの最初の5行表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss, accuracy_score#混同行列を計算するライブラリ\n",
    "from sklearn.model_selection import KFold#ホールドアウト、交差検証(cross validation)するライブラリ\n",
    "import lightgbm as lgb#LightGBMのライブラリ\n",
    "import numpy as np#数値計算するライブラリ\n",
    "\n",
    "# 各foldのスコアを保存する空のリスト\n",
    "scores_accuracy = []#[]:正解率用のリスト\n",
    "scores_logloss = []#[]:評価関数LogLoss用のリスト\n",
    "\n",
    "# クロスバリデーション(k-fold cross validation)を行う\n",
    "# 学習データを4つに分割し、うち1つをバリデーションデータとすることを、バリデーションデータを変えて繰り返す\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)#n_splits:分割,shuffle:シャッフルするかどうか,random_state:何度実行しても同じランダムな分割になるように固定する値。指定しないと毎回分割データが変わり結果が異なってしまう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "確認用【for tr_idx, va_idx in kf.split(train_x):】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 1#カウント用の数字\n",
    "for tr_idx, va_idx in kf.split(train_x):#クロスバリデーションで分割。for：繰り返し(n_splitsの回数繰り返す),in：どんな条件で繰り返すか\n",
    "    print('########{}回目:データ数 tr:{},va:{}########'.format(cnt,len(tr_idx),len(va_idx)))#print:文字列を出力、len([]):リストの長さ\n",
    "    print('tr_idx:{}'.format(tr_idx))#'{}'.format(変数)\n",
    "    print('va_idx:{}'.format(va_idx))\n",
    "    cnt += 1#cntを1足す。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初期ハイパーパラメータ初期設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tr_idx, va_idx in kf.split(train_x):#クロスバリデーションで分割。for：繰り返し(n_splitsの回数繰り返す),in：どんな条件で繰り返すか\n",
    "    # 学習データを学習データとバリデーションデータに分ける\n",
    "    tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]#iloc[]:行番号、列番号を指定して要素を取り出す\n",
    "    tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n",
    "\n",
    "    # モデルの学習を行う\n",
    "    model = lgb.LGBMClassifier() # LightGBMのモデルを定義。分類:LGBMClassifier(),回帰:LGBMRegressor()\n",
    "    model.fit(tr_x, tr_y) # モデルの学習:fit(説明変数,目的変数)\n",
    "\n",
    "    # テストデータの予測クラス (予測クラス(0 or 1)を返す)\n",
    "    y_pred = model.predict(va_x)\n",
    "    # テストデータのクラス予測確率 (各クラスの予測確率 [クラス0の予測確率,クラス1の予測確率] を返す)\n",
    "    y_pred_prob = model.predict_proba(va_x)    \n",
    "\n",
    "    # モデル評価\n",
    "    # acc : 正答率\n",
    "    accuracy = accuracy_score(va_y,y_pred)#accuracy_score(正解値,予測値)\n",
    "    print('accuracy :',accuracy)\n",
    "\n",
    "    # 評価関数LogLoss \n",
    "    logloss =  log_loss(va_y,y_pred_prob) # 引数 : log_loss(正解クラス,[クラス0の予測確率,クラス1の予測確率])\n",
    "    print('logloss :', logloss)\n",
    "\n",
    "    # 各foldのスコアを保存する\n",
    "    scores_logloss.append(logloss)#append:listに追加\n",
    "    scores_accuracy.append(accuracy)\n",
    "\n",
    "# 各foldのスコアの平均を出力する\n",
    "print('scores_logloss:{}'.format(scores_logloss))\n",
    "print('scores_accuracy:{}'.format(scores_accuracy))\n",
    "logloss = np.mean(scores_logloss)#各foldの評価関数LogLossの平均値\n",
    "accuracy = np.mean(scores_accuracy)#各foldの正解率の平均値\n",
    "print(\"===予測モデルの評価指標(平均スコア)===\")\n",
    "print(f'logloss: {logloss:.4f}, accuracy: {accuracy:.4f}')#f'':変数をそのまま指定できる,{*.4f}:小数点以下の桁数(この場合4桁)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "グリッドリサーチでハイパーパラメータ設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV#グリッドサーチのライブラリ\n",
    "\n",
    "model = lgb.LGBMClassifier() # LightGBMのモデルを定義\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)#n_splits:分割,shuffle:シャッフルするかどうか,random_state:何度実行しても同じランダムな分割になるように固定する値。指定しないと毎\n",
    "\n",
    "# パラメーターを設定する\n",
    "param_grid = {\"max_depth\": [5,6,7,8,9],#木構造の深さを限定するための変数．データが少ないときに過学習を防ぐために設定する\n",
    "              \"learning_rate\" : [0.05,0.06,0.07,0.08,0.09],#学習率\n",
    "              'num_leaves': [5,6,7,8,9]#木にある分岐の個数\n",
    "             }\n",
    "# パラメータチューニングをグリッドサーチで行うために設定する\n",
    "## このGridSearchCV には注意が必要 scoring は そのスコアを基準にして最適化する\n",
    "grid = GridSearchCV(estimator = model,#モデル\n",
    "                    param_grid = param_grid,#パラメータ\n",
    "                    scoring = 'accuracy',#評価指標\n",
    "                    cv = kf,#交差検定の回数\n",
    "                    verbose=1,#verbose=1:一定の間隔でログ表示，verbose=2:テスト毎にログ表示，verbose=3:テスト毎にスコアも含めてログ表示\n",
    "                    return_train_score = True,#False場合、トレーニングスコアを含まない\n",
    "                    n_jobs = -1)#同時実行数(-1にするとコア数で同時実行)。#コア数とはコンピュータのCPUに内蔵された稼働するプロセッサコアの数。 コアの数だけ複数のプログラムを並列に動作させられる\n",
    "\n",
    "grid.fit(c)#学習\n",
    "\n",
    "print(\"===予測モデルの評価指標(平均スコア)===\")\n",
    "print(\"最も良いパラメータ:{}\".format(grid.best_params_))\n",
    "print(f'accuracy: {grid.best_score_:.4f}')#f'':変数をそのまま指定できる,{*.4f}:小数点以下の桁数(この場合4桁)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "アンサンブル学習(https://potesara-tips.com/ensemble-voting/#toc10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb#機械学習モデル：xgboost\n",
    "import lightgbm as lgb#機械学習モデル：lightgbm\n",
    "from catboost import CatBoost#機械学習モデル：catboost\n",
    "from catboost import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "#関数とは\n",
    "#def 関数名(引数1, 引数2, ...):\n",
    "#    処理\n",
    "#    return 戻り値\n",
    "\n",
    "def Add(a, b):\n",
    "    x = a + b\n",
    "    return x\n",
    "\n",
    "x = Add(4, 1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_train(X_train, y_train, X_eval, y_eval, X_test, y_test):#def:関数定義\n",
    "    # 学習用データセット作成\n",
    "    xgb_train = xgb.DMatrix(X_train, label=y_train)#X_train_cv:目的変数のデータ,y_train_cv:説明変数のデータ\n",
    "    # 検証用データセット作成\n",
    "    xgb_eval = xgb.DMatrix(X_eval, label=y_eval)#X_eval_cv:目的変数のデータ,y_eval_cv:説明変数のデータ\n",
    "    # テスト用データセット作成\n",
    "    xgb_test = xgb.DMatrix(X_test, label=y_test)#X_test:目的変数のデータ,y_test:説明変数のデータ\n",
    "    \n",
    "    # パラメータを設定\n",
    "    xgb_params = {\n",
    "        'objective': 'multi:softprob',  # 多値分類問題\n",
    "        #softprob:予測するそれぞれのクラスの確率値を出力[0.7,0.2,0.1]\n",
    "        #softmax:予測が最大確率となっている1クラスを出力[0.7,0.2,0.1]→0    \n",
    "        'num_class': 3,                 # 目的変数のクラス数\n",
    "        'learning_rate': 0.1,           # 学習率\n",
    "        'eval_metric': 'mlogloss'       # 学習用の指標 (Multiclass logloss)\n",
    "    }\n",
    "\n",
    "    # 学習\n",
    "    evals = [(xgb_train, 'train'), (xgb_eval, 'eval')] # 学習に用いる検証用データ\n",
    "    evaluation_results = {}                            # 学習の経過を保存する箱\n",
    "    bst = xgb.train(xgb_params,                        # 上記で設定したパラメータ\n",
    "                    xgb_train,                         # 使用するデータセット\n",
    "                    num_boost_round=200,               # 学習の回数\n",
    "                    early_stopping_rounds=10,          # アーリーストッピング\n",
    "                    evals=evals,                       # 学習経過で表示する名称\n",
    "                    evals_result=evaluation_results,   # 上記で設定した検証用データ\n",
    "                    verbose_eval=0                     # 学習の経過の表示(非表示)\n",
    "                    )\n",
    "    \n",
    "    # テストデータで予測\n",
    "    y_pred = bst.predict(xgb_test, ntree_limit=bst.best_ntree_limit)#各クラスの予想確率#一番良いモデルで予測する\n",
    "    y_pred_max = np.argmax(y_pred, axis=1)#最大値のインデックス（位置）を取得\n",
    "\n",
    "    accuracy = accuracy_score(y_test.values.tolist(), y_pred_max)#クラス分類結果を評価\n",
    "    print('XGBoost Accuracy:', accuracy)\n",
    "    \n",
    "    return(bst, y_pred_max, accuracy)#return:出力、bst:モデル、y_pred_max:予測値,acuracy:正解率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm_train(X_train, y_train, X_eval, y_eval, X_test, y_test):\n",
    "    # データを格納する\n",
    "    # 学習用\n",
    "    lgb_train = lgb.Dataset(X_train, y_train,free_raw_data=False)\n",
    "    # 検証用\n",
    "    lgb_eval = lgb.Dataset(X_eval, y_eval, reference=lgb_train,free_raw_data=False)\n",
    "    # パラメータを設定\n",
    "    params = {'task': 'train',                # レーニング ⇔　予測predict\n",
    "              'boosting_type': 'gbdt',        # 勾配ブースティング\n",
    "              'objective': 'multiclass',      # 目的関数：多値分類、マルチクラス分類\n",
    "              'metric': 'multi_logloss',      # 検証用データセットで、分類モデルの性能を測る指標\n",
    "              'num_class': 3,                 # 目的変数のクラス数\n",
    "              'learning_rate': 0.1,           # 学習率（初期値0.1）\n",
    "              'num_leaves': 23,               # 決定木の複雑度を調整（初期値31）\n",
    "              'min_data_in_leaf': 1,          # データの最小数（初期値20）\n",
    "              'verbose': -1,\n",
    "             }\n",
    "\n",
    "    # 学習\n",
    "    evaluation_results = {}                                # 学習の経過を保存する箱\n",
    "    model = lgb.train(params,                              # 上記で設定したパラメータ\n",
    "                      lgb_train,                           # 使用するデータセット\n",
    "                      num_boost_round=200,                 # 学習の回数\n",
    "                      valid_names=['train', 'valid'],      # 学習経過で表示する名称\n",
    "                      valid_sets=[lgb_train, lgb_eval],    # モデルの検証に使用するデータセット\n",
    "                      evals_result=evaluation_results,     # 学習の経過を保存\n",
    "                      early_stopping_rounds=10,            # アーリーストッピングの回数\n",
    "                      verbose_eval=0)                      # 学習の経過を表示する刻み（非表示）\n",
    "\n",
    "    # テストデータで予測\n",
    "    y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "    y_pred_max = np.argmax(y_pred, axis=1)\n",
    "    # Accuracy の計算\n",
    "    accuracy = accuracy_score(y_test, y_pred_max)\n",
    "    print('LightGBM Accuracy:', accuracy)\n",
    "    \n",
    "    return(model, y_pred_max, accuracy)#return:出力、model:モデル、y_pred_max:予測値,acuracy:正解率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catboost_train(X_train, y_train, X_eval, y_eval, X_test, y_test):\n",
    "    # データを格納する\n",
    "    # 学習用\n",
    "    CatBoost_train = Pool(X_train, label=y_train)\n",
    "    # 検証用\n",
    "    CatBoost_eval = Pool(X_eval, label=y_eval)\n",
    "    \n",
    "    # パラメータを設定\n",
    "    params = {        \n",
    "        'loss_function': 'MultiClass',    # 多値分類問題\n",
    "        'num_boost_round': 1000,          # 学習の回数\n",
    "        'early_stopping_rounds': 10       # アーリーストッピングの回数\n",
    "    }\n",
    "    \n",
    "    # 学習\n",
    "    catb = CatBoost(params)\n",
    "    catb.fit(CatBoost_train, eval_set=[CatBoost_eval], verbose=False)\n",
    "\n",
    "    # テストデータで予測\n",
    "    y_pred = catb.predict(X_test, prediction_type='Probability')\n",
    "    y_pred_max = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    # Accuracy の計算\n",
    "    accuracy = accuracy_score(y_test, y_pred_max)\n",
    "    #accuracy = sum(y_test == y_pred_max) / len(y_test)\n",
    "    print('CatBoost Accuracy:', accuracy)\n",
    "    \n",
    "    return(catb, y_pred_max, accuracy)#return:出力、catb:モデル、y_pred_max:予測値,acuracy:正解率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb#機械学習モデル：xgboost\n",
    "import lightgbm as lgb#機械学習モデル：lightgbm\n",
    "from catboost import CatBoost#機械学習モデル：catboost\n",
    "from catboost import Pool\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 学習データとテストデータに分ける\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_x, train_y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=1,\n",
    "                                                    stratify=train_y)\n",
    "\n",
    "# Voting\n",
    "# 各3つのモデルを保存するリスト\n",
    "xgb_models = []\n",
    "lgbm_models = []\n",
    "catb_models = []\n",
    "# 各3つのモデルの正答率を保存するリスト\n",
    "xgb_accuracies = []\n",
    "lgbm_accuracies = []\n",
    "catb_accuracies = []\n",
    "# 学習のカウンター\n",
    "loop_counts = 1\n",
    "\n",
    "# 各3つのモデルの予測を保存する配列の初期化（kfoldの3*3モデル）\n",
    "first_preds = np.zeros((len(y_test), 3*3))\n",
    "\n",
    "# 学習データの数だけの数列（0行から最終行まで連番）\n",
    "row_no_list = list(range(len(y_train)))\n",
    "\n",
    "# KFoldクラスをインスタンス化（これを使って3分割する）\n",
    "K_fold = StratifiedKFold(n_splits=3, shuffle=True,  random_state= 1)\n",
    "\n",
    "# KFoldクラスで分割した回数だけ実行（ここでは3回）\n",
    "for train_no, eval_no in K_fold.split(row_no_list, y_train):\n",
    "    # ilocで取り出す行を指定\n",
    "    X_train_cv = X_train.iloc[train_no, :]\n",
    "    y_train_cv = y_train.iloc[train_no]\n",
    "    X_eval_cv = X_train.iloc[eval_no, :]\n",
    "    y_eval_cv = y_train.iloc[eval_no]\n",
    "\n",
    "    # XGBoostの学習を実行\n",
    "    bst, bst_pred, bst_accuracy = xgb_train(X_train_cv, y_train_cv,\n",
    "                                            X_eval_cv, y_eval_cv, \n",
    "                                            X_test, y_test)\n",
    "    # LIghtGBMの学習を実行\n",
    "    model, model_pred, model_accuracy = lgbm_train(X_train_cv, y_train_cv,\n",
    "                                                   X_eval_cv, y_eval_cv,\n",
    "                                                   X_test, y_test)\n",
    "    # CatBoostの学習を実行\n",
    "    catb, catb_pred, catb_accuracy = catboost_train(X_train_cv, y_train_cv,\n",
    "                                                    X_eval_cv, y_eval_cv,\n",
    "                                                    X_test, y_test)\n",
    "\n",
    "    # 学習が終わったモデルをリストに入れておく\n",
    "    xgb_models.append(bst) \n",
    "    lgbm_models.append(model) \n",
    "    catb_models.append(catb) \n",
    "\n",
    "    # 学習が終わったモデルの正答率をリストに入れておく\n",
    "    xgb_accuracies.append(bst_accuracy) \n",
    "    lgbm_accuracies.append(model_accuracy) \n",
    "    catb_accuracies.append(catb_accuracy) \n",
    "\n",
    "    # 学習が終わったモデルの予測をリストに入れておく\n",
    "    first_preds[:, loop_counts-1] = bst_pred\n",
    "    first_preds[:, loop_counts-1 + 3] = model_pred\n",
    "    first_preds[:, loop_counts-1 + 6] = catb_pred\n",
    "\n",
    "    # 実行回数のカウント\n",
    "    loop_counts += 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単独のモデルでの、テストデータの正答率\n",
    "print('XGBoost Accuracy: ', np.array(xgb_accuracies).mean())#平均値算出\n",
    "print('LightGBM Accuracy: ', np.array(lgbm_accuracies).mean())#平均値算出\n",
    "print('CatBoost Accuracy: ', np.array(catb_accuracies).mean())#平均値算出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# アンサンブルのモデルでの、テストデータの正答率\n",
    "import itertools\n",
    "# データ格納用のnumpy行列を作成\n",
    "first_preds_max = pd.DataFrame(np.zeros((len(y_test), 3)))\n",
    "\n",
    "# 予測したクラスのデータをpandas.DataFrameに入れる\n",
    "df_first_preds = pd.DataFrame(first_preds)\n",
    "\n",
    "# 各列（0,1,2）に、そのクラスを予測したモデルの数を入れる\n",
    "first_preds_max.iloc[:, 0] = (df_first_preds == 0).sum(axis=1)\n",
    "first_preds_max.iloc[:, 1] = (df_first_preds == 1).sum(axis=1)\n",
    "first_preds_max.iloc[:, 2] = (df_first_preds == 2).sum(axis=1)\n",
    "\n",
    "# 各行で、そのクラスを予測したモデルの数が最も多いクラスを得る\n",
    "pred_max = np.argmax(np.array(first_preds_max), axis=1)\n",
    "\n",
    "# Accuracy を計算する\n",
    "#accuracy = sum(y_test.values.tolist() == pred_max) / len(y_test)\n",
    "accuracy = accuracy_score(y_test, pred_max)\n",
    "print('accuracy:', accuracy)\n",
    "#df_accuracy = pd.DataFrame({'va_y': list(itertools.chain.from_iterable(y_test.values)),\n",
    "#                            'y_pred_max': pred_max})\n",
    "#print(pd.crosstab(df_accuracy['va_y'], df_accuracy['y_pred_max']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
